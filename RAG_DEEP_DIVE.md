# RAG (Retrieval Augmented Generation) Deep Dive

## What is RAG?

RAG is a technique that enhances AI responses by retrieving relevant context from a knowledge base before generating a response. Instead of relying solely on the AI's training data, RAG allows the AI to access and use current, specific information from your codebase.

## How RAG Works in Code Review

### 1. **Indexing Phase** (Done Once Per Repository)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INDEXING: Building the Knowledge Base                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Fetch All Files
â”œâ”€ Call Azure DevOps API to get repository file tree
â”œâ”€ Filter out binary files, node_modules, etc.
â””â”€ Result: List of .cs, .py, .rs files

Step 2: Chunk Each File
â”œâ”€ Split large files into 100-line chunks with 10-line overlap
â”œâ”€ Why chunks? Large files don't fit in embedding models
â””â”€ Result: ~1000 code chunks from 50 files

Step 3: Generate Embeddings
â”œâ”€ For each chunk, call OpenAI Embedding API
â”œâ”€ Converts code text â†’ 1536-dimensional vector
â”œâ”€ Vectors capture semantic meaning of code
â””â”€ Result: Each chunk has a float[1536] embedding

Step 4: Store in Vector Database
â”œâ”€ In-memory dictionary (repositoryId â†’ chunks)
â”œâ”€ Production: Use Qdrant, Pinecone, or Azure AI Search
â””â”€ Result: Searchable knowledge base
```

### 2. **Retrieval Phase** (Done For Each File Review)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RETRIEVAL: Finding Relevant Context                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Build Search Query
â”œâ”€ Extract added lines from PR diff
â”œâ”€ Include file name and context
â””â”€ Example: "public class UserService { async Task<User> GetUser..."

Step 2: Generate Query Embedding
â”œâ”€ Convert search query â†’ 1536-dimensional vector
â””â”€ Same embedding model as indexing

Step 3: Semantic Search (Cosine Similarity)
â”œâ”€ Compare query vector with all chunk vectors
â”œâ”€ Formula: similarity = dot(A, B) / (||A|| * ||B||)
â”œâ”€ Values range from -1 to 1 (1 = identical)
â””â”€ Filter results with similarity > 0.7

Step 4: Rank and Return
â”œâ”€ Sort by similarity score descending
â”œâ”€ Take top 3-5 most relevant chunks
â””â”€ Return code snippets with metadata
```

### 3. **Augmentation Phase** (Adding Context to AI Prompt)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AUGMENTATION: Enhancing the AI Prompt                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Original Prompt:
"Review this code change: [diff]"

Augmented Prompt:
"Review this code change: [diff]

## Relevant Codebase Context

### Similar code (relevance: 0.85)
Location: Services/AuthService.cs:L45-L145
```csharp
public class AuthService {
  // Similar authentication patterns
}
```

### Similar code (relevance: 0.78)
Location: Controllers/UserController.cs:L22-L122
```csharp
public class UserController {
  // Similar API patterns
}
```

## Related Files (Dependencies)
### Models/User.cs
[First 20 lines of dependent file]
"
```

## Why RAG is Powerful

### Without RAG:
- AI only knows general programming patterns
- No context about YOUR codebase
- Can't see similar implementations
- May suggest patterns that conflict with your style

### With RAG:
- âœ… AI sees how you handle authentication in other files
- âœ… Knows your error handling patterns
- âœ… Understands your naming conventions
- âœ… Can reference actual implementations from your repo
- âœ… Gives context-aware suggestions

## Example: Real RAG in Action

**File Being Reviewed:** `Services/PaymentService.cs`
```csharp
+ public async Task<Payment> ProcessPayment(Order order) {
+   var result = await _api.Charge(order.Total);
+   return new Payment { Status = result.Status };
+ }
```

**RAG Retrieves:**
1. `Services/OrderService.cs` (similarity: 0.89)
   - Shows error handling pattern: try/catch with logging

2. `Services/EmailService.cs` (similarity: 0.82)
   - Shows null checks before API calls

3. `Models/Payment.cs` (dependency)
   - Shows Payment model has more fields than just Status

**AI Review With RAG Context:**
"âš ï¸ Missing error handling. Based on `OrderService.cs`, all API calls should be wrapped in try/catch with logging. Also, `Payment.cs` shows the model has `TransactionId` and `Timestamp` fields that should be populated."

**Without RAG:**
"âœ… Looks good. Consider adding error handling." (generic advice)

## Implementation Details

### Vector Similarity Math
```csharp
// Cosine Similarity measures angle between vectors
// Range: -1 (opposite) to 1 (identical)
double CosineSimilarity(float[] A, float[] B) {
    double dotProduct = 0;
    double magA = 0, magB = 0;

    for (int i = 0; i < A.Length; i++) {
        dotProduct += A[i] * B[i];
        magA += A[i] * A[i];
        magB += B[i] * B[i];
    }

    return dotProduct / (Math.Sqrt(magA) * Math.Sqrt(magB));
}
```

### Chunking Strategy
```
File: 500 lines
Chunk Size: 100 lines
Overlap: 10 lines

Chunks:
1. Lines 1-100
2. Lines 91-190    (10 line overlap with chunk 1)
3. Lines 181-280   (10 line overlap with chunk 2)
4. Lines 271-370
5. Lines 361-460
6. Lines 451-500

Why overlap? Prevents splitting related code across chunks.
```

### Embedding Cost & Performance

**Indexing (one-time per repo):**
- 50 files Ã— 5 chunks/file = 250 chunks
- 250 embedding API calls
- Cost: ~$0.01 (text-embedding-3-small)
- Time: ~30 seconds

**Retrieval (per file review):**
- 1 embedding API call for query
- In-memory vector search: <10ms
- Cost: ~$0.0001
- Time: ~100ms total

## Current Implementation Status

âœ… **Implemented:**
- Vector storage (in-memory dictionary)
- Semantic search with cosine similarity
- Chunking with overlap
- Dependency parsing (.cs, .py, .rs)
- Embedding generation

âŒ **Not Yet Working:**
- Repository indexing (returns 0 files)
- Need to fix Azure DevOps API call
- Need to trigger indexing when PR is opened

ğŸ”§ **Next Steps:**
1. Fix GetRepositoryItemsAsync to return actual files
2. Trigger IndexRepositoryAsync when reviewing a PR
3. Add comprehensive logging at each step
4. Test with real codebase
